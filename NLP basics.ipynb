{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17403377-76ee-4db7-bb3b-a5ecc89559ac",
   "metadata": {},
   "source": [
    "### Text Preprocessing\r\n",
    "Preprocessing involves cleaning and preparing text data for analysis. Common techniques include:\r\n",
    "- **Tokenization**: Splitting text into words or sentences.\r\n",
    "- **Stopword Removal**: Removing common words (e.g., \"and\", \"the\") that may not add value.\r\n",
    "- **Stemming**: Reducing words to their base or root form (e.g., \"running\" to \"run\").\r\n",
    "- **Lemmatization**: Similar to stemming but considers the context of the word (e.g., \"better\" to \"goo\n",
    "\n",
    "- ![Text Processing](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pzjECYWP8WOWhwfCjebZVw.png)\n",
    "\n",
    "d\").\r\n",
    "\r\n",
    "In the code below:\r\n",
    "1. We use NLTK (Natural Language Toolkit) for tokenization, stopword removal, stemming, and lemmatization.\r\n",
    "2. Tokenization splits the text into individual words.\r\n",
    "3. Stopwords are filtered out to keep only meaningful words.\r\n",
    "4. Stemming and Lemmatization reduce words to their root forms, though they operate dimmatization).\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ec7f12-648d-45de-b98c-34561c94e57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rajka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rajka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rajka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'running', 'cats', 'are', 'jumping', 'over', 'the', 'lazy', 'dog', '.']\n",
      "Filtered Tokens: ['running', 'cats', 'jumping', 'lazy', 'dog', '.']\n",
      "Stemmed Tokens: ['run', 'cat', 'jump', 'lazi', 'dog', '.']\n",
      "Lemmatized Tokens: ['running', 'cat', 'jumping', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "text = \"The running cats are jumping over the lazy dog.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Stopword removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Filtered Tokens:\", filtered_tokens)\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc05c4-96ed-4f04-b45e-7ad9fa0a5f63",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)\n",
    "The Bag of Words model represents text data as a set of words without considering the order. Each document is transformed into a vector based on word frequency.\n",
    "\n",
    "![BOW](https://alasheep.com/assets/static/bow_representation.6acf7b4.1e3b4dbd9fb1beacb8309a733db4f517.jpeg)\n",
    "\n",
    "In the code below:\n",
    "1. We use `CountVectorizer` from `sklearn` to convert text data into a BoW representation.\n",
    "2. Each document is transformed into a vector, where each element counts the occurrences of a specific word.\n",
    "3. The resulting array shows the word counts for each document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce0d22e4-2f5f-494b-9ff0-83f30056ac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Array:\n",
      " [[1 1 0 1 1 0]\n",
      " [1 0 1 1 1 1]]\n",
      "Feature Names: ['are' 'cats' 'dogs' 'great' 'pets' 'too']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\"Cats are great pets.\", \"Dogs are great pets too.\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to array for better visualization\n",
    "boW_array = X.toarray()\n",
    "print(\"BoW Array:\\n\", boW_array)\n",
    "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b167e8-704d-4430-8272-bba7fc546a6a",
   "metadata": {},
   "source": [
    "### Interpretation of Bag of Words (BoW) Result\n",
    "\n",
    "- **BoW Array**: Each row corresponds to a document, and each column represents the count of a word in that document.\n",
    "\n",
    "  - **Document 1** (`\"Cats are great pets.\"`):\n",
    "    - The words \"are\", \"great\", \"pets\", and \"cats\" each appear once, so they have counts of `1` in this row.\n",
    "    - The words \"dogs\" and \"too\" do not appear in Document 1, so their counts are `0`.\n",
    "\n",
    "  - **Document 2** (`\"Dogs are great pets too.\"`):\n",
    "    - The words \"dogs\", \"are\", \"great\", \"pets\", and \"too\" each appear once, so they have counts of `1`.\n",
    "    - The word \"cats\" does not appear in Document 2, so its count is `0`.\n",
    "\n",
    "- **Feature Names**: These are the unique words (vocabulary) extracted from both documents. Each word aligns with a column in the BoW array, allowing us to interpret the counts in each document.\n",
    "\n",
    "This BoW representation is useful for basic text analysis, but it does not capture word importance across documents (like TF-IDF does) or semantic relationships between words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e10bda-09ec-43ab-bca6-4a504a527cd4",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "TF-IDF improves upon the BoW model by considering the importance of words in the context of the entire corpus.\n",
    "\n",
    "![BOW](https://www.romainberg.com/wp-content/uploads/TF_IDF-final-980x381.png)\n",
    "\n",
    "In the code below:\n",
    "1. We use `TfidfVectorizer` from `sklearn` to create a TF-IDF matrix for the documents.\n",
    "2. Term Frequency (TF) counts how frequently each word appears, while Inverse Document Frequency (IDF) reduces the weight of common words.\n",
    "3. The resulting matrix provides a more informative representation of word importance across the documents.\n",
    "\n",
    "![formula](https://ptime.s3.ap-northeast-1.amazonaws.com/media/natural_language_processing/text_feature_Engineering/tf-idf-formula.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965bc848-77c9-4511-b6e7-d43ae1860d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Array:\n",
      " [[0.44832087 0.63009934 0.         0.44832087 0.44832087 0.        ]\n",
      " [0.37930349 0.         0.53309782 0.37930349 0.37930349 0.53309782]]\n",
      "Feature Names: ['are' 'cats' 'dogs' 'great' 'pets' 'too']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to array for visualization\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "print(\"TF-IDF Array:\\n\", tfidf_array)\n",
    "print(\"Feature Names:\", tfidf_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea765b6-18eb-40c1-af88-0dce14747712",
   "metadata": {},
   "source": [
    "### Interpretation of TF-IDF Result\n",
    "\n",
    "- **TF-IDF Array**: Each row in the TF-IDF array corresponds to a document, and each column represents the TF-IDF score of a specific word in that document. These scores indicate the relative importance of each word in each document based on how often it appears in this document compared to the whole document set.\n",
    "\n",
    "- **Document 1**: `[\"Cats are great pets.\"]`\n",
    "  - The words \"cats\" and \"pets\" are assigned higher TF-IDF scores because they are important within this document relative to the rest of the corpus.\n",
    "  \n",
    "- **Document 2**: `[\"Dogs are great pets too.\"]`\n",
    "  - The words \"dogs\" and \"pets\" receive higher scores here, emphasizing their relevance and distinguishing this document from others in the set.\n",
    "\n",
    "- **Feature Names**: These are the unique vocabulary terms identified from the documents. They align with the columns in the TF-IDF array, helping us match each score in the array to the corresponding word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8526457-5a4d-4156-b301-efda4a5f38c5",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "Word embeddings like Word2Vec or GloVe convert words into dense vectors, capturing semantic relationships.\n",
    "\n",
    "In the code below:\n",
    "1. We use the `Word2Vec` model from the `gensim` library to create embeddings for each word.\n",
    "2. The model learns the vector representation based on word co-occurrences within a defined window.\n",
    "3. The output shows the vector for a sample word, representing its learned semantic meaning in multi-dimensional space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eb4df3c-1dac-498c-9517-e8b9600bb605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector for 'cats': [-0.0960355   0.05007293 -0.08759586 -0.04391825 -0.000351   -0.00296181\n",
      " -0.0766124   0.09614743  0.04982058  0.09233143]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences for training\n",
    "sentences = [[\"cats\", \"are\", \"great\"], [\"dogs\", \"are\", \"also\", \"great\"]]\n",
    "model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, workers=4)\n",
    "\n",
    "# Get word vectors\n",
    "cat_vector = model.wv['cats']\n",
    "print(\"Word Vector for 'cats':\", cat_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6193098-31c6-45ce-885f-704e837ab5dd",
   "metadata": {},
   "source": [
    "### Interpretation of Word2Vec Result\n",
    "\n",
    "- **Word Vector for 'cats'**: The output is a 10-dimensional vector that represents \"cats\" in the semantic space learned by the Word2Vec model.\n",
    "  - Each element in this vector captures some aspect of the wordâ€™s context based on the training sentences.\n",
    "  \n",
    "- **Semantic Similarity**: Similar words (words that appear in similar contexts) will have similar vector representations.\n",
    "  - For example, \"cats\" and \"dogs\" might have similar vectors since they share the context of \"are\" and \"great\" in the training sentences.\n",
    "\n",
    "- **Use of Word Vectors**: Word vectors generated by Word2Vec are useful for capturing semantic relationships, allowing words with similar meanings or contexts to be compared or clustered together.\n",
    "  \n",
    "This is beneficial for various NLP tasks, such as identifying synonyms, grouping related words, or analyzing word similarity in a specific context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a803ff-da8b-4307-bce9-826646a02977",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "NER is used to identify and classify named entities in text (e.g., people, organizations).\n",
    "\n",
    "In the code below:\n",
    "1. We use the `spaCy` library to detect named entities in a sentence.\n",
    "2. Entities like organizations, locations, and monetary values are extracted and labeled.\n",
    "3. This technique is helpful for extracting structured information from unstructured text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba4092cd-dd4c-4b27-a725-339c25303fb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'spacy' has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspc\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the English NLP model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spc\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApple is looking at buying U.K. startup for $1 billion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Identify named entities\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'spacy' has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "import spacy as spc\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spc.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# Identify named entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7a0d41-ca91-4d53-9a78-5d9c5c8d4a12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
